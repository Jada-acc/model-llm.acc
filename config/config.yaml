model:
  name: "autonomous-llm"
  version: "0.1.0"
  architecture:
    model_type: "transformer"
    base_model: "gpt2"  # or any other base model
    hidden_size: 768
    num_attention_heads: 12
    num_hidden_layers: 12
    intermediate_size: 3072
    max_sequence_length: 512

data:
  input:
    batch_size: 32
    num_workers: 4
    shuffle: true
  storage:
    type: "distributed"
    format: "parquet"
    compression: "snappy"
    cloud_path: "s3://your-bucket/data"

training:
  optimizer:
    name: "adam"
    learning_rate: 1.0e-4
    weight_decay: 0.01
  scheduler:
    name: "linear"
    warmup_steps: 1000
  max_epochs: 10
  gradient_accumulation_steps: 4
  mixed_precision: true
  checkpointing:
    save_steps: 1000
    save_total_limit: 5

cloud:
  provider: "aws"  # or "gcp", "azure"
  region: "us-west-2"
  instance_type: "ml.p3.2xlarge"
  storage:
    bucket_name: "your-bucket"
    credentials_path: "~/.aws/credentials"

monitoring:
  metrics:
    - loss
    - accuracy
    - gpu_utilization
    - memory_usage
  logging:
    level: "INFO"
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  dashboard:
    port: 8501
    update_interval: 5

blockchain:
  network: "ethereum"
  contract_address: ""  # Your smart contract address
  provider_url: "http://localhost:8545"  # Local or remote node URL
  gas_limit: 3000000

api:
  host: "0.0.0.0"
  port: 8000
  debug: false
  cors_origins: ["*"]
  rate_limit: 100  # requests per minute 
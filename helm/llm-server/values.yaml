# Default values for llm-server
replicaCount: 1

image:
  repository: alpine
  tag: "3.18"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 8000

resources:
  limits:
    cpu: "200m"
    memory: "128Mi"
  requests:
    cpu: "100m"
    memory: "64Mi"

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

modelConfig:
  name: "gpt2"
  version: "latest"
  maxBatchSize: 32
  maxMemory: 0.9
  maxCacheSize: 5

storage:
  modelStorage:
    size: "1Gi"
    storageClass: "hostpath"
  cacheStorage:
    enabled: true

monitoring:
  enabled: false
  prometheusRule:
    enabled: false
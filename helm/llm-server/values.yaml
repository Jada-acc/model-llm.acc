# Default values for llm-server
replicaCount: 3

image:
  repository: python
  tag: "3.9-slim"
  pullPolicy: IfNotPresent

service:
  type: LoadBalancer
  port: 80
  targetPort: 8000

resources:
  limits:
    cpu: "2"
    memory: "1Gi"
  requests:
    cpu: "500m"
    memory: "512Mi"

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

modelConfig:
  name: "gpt2"
  version: "latest"
  maxBatchSize: 32
  maxMemory: 0.9
  maxCacheSize: 5

storage:
  modelStorage:
    size: "1Gi"
    storageClass: "hostpath"
  cacheStorage:
    enabled: true
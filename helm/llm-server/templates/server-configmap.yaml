apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "llm-server.fullname" . }}-server
  labels:
    {{- include "llm-server.labels" . | nindent 4 }}
data:
  server.py: |
    from http.server import HTTPServer, BaseHTTPRequestHandler
    import json
    import signal
    import sys
    import time
    from prometheus_client import start_http_server, Counter, Histogram, Gauge

    # Prometheus metrics
    REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['path', 'method', 'status'])
    REQUEST_LATENCY = Histogram('http_request_duration_seconds', 'HTTP request latency')
    
    # Custom metrics
    MODEL_LOAD_TIME = Histogram('model_load_time_seconds', 'Time to load model')
    INFERENCE_LATENCY = Histogram('inference_latency_seconds', 'Model inference latency')
    MODEL_MEMORY_USAGE = Gauge('model_memory_usage_bytes', 'Current model memory usage')
    BATCH_SIZE = Histogram('batch_size', 'Batch sizes of requests')
    CACHE_HIT_RATIO = Gauge('cache_hit_ratio', 'Cache hit ratio')
    QUEUE_SIZE = Gauge('request_queue_size', 'Number of requests in queue')

    class Handler(BaseHTTPRequestHandler):
        def do_GET(self):
            start_time = time.time()
            try:
                if self.path == '/health':
                    self.send_response(200)
                    self.send_header('Content-Type', 'application/json')
                    self.end_headers()
                    self.wfile.write(json.dumps({"status": "healthy"}).encode())
                    REQUEST_COUNT.labels(path='/health', method='GET', status=200).inc()
                else:
                    self.send_response(404)
                    self.end_headers()
                    REQUEST_COUNT.labels(path=self.path, method='GET', status=404).inc()
            finally:
                REQUEST_LATENCY.observe(time.time() - start_time)

        def do_POST(self):
            start_time = time.time()
            try:
                if self.path == '/predict':
                    content_length = int(self.headers.get('Content-Length', 0))
                    if content_length > 0:
                        post_data = self.rfile.read(content_length)
                        request = json.loads(post_data.decode('utf-8'))
                        
                        # Simulate model inference
                        inference_start = time.time()
                        time.sleep(0.1)  # Simulate processing
                        INFERENCE_LATENCY.observe(time.time() - inference_start)
                        
                        # Update metrics
                        BATCH_SIZE.observe(len(request.get('inputs', [])))
                        MODEL_MEMORY_USAGE.set(1024 * 1024 * 100)  # Simulate 100MB usage
                        CACHE_HIT_RATIO.set(0.8)  # Simulate 80% cache hit
                        QUEUE_SIZE.set(5)  # Simulate 5 requests in queue
                        
                        response = {
                            "predictions": ["Sample prediction"],
                            "model": "gpt2",
                            "version": "latest"
                        }
                        
                        self.send_response(200)
                        self.send_header('Content-Type', 'application/json')
                        self.end_headers()
                        self.wfile.write(json.dumps(response).encode())
                        REQUEST_COUNT.labels(path='/predict', method='POST', status=200).inc()
                    else:
                        self.send_response(400)
                        self.send_header('Content-Type', 'application/json')
                        self.end_headers()
                        self.wfile.write(json.dumps({"error": "Empty request body"}).encode())
                        REQUEST_COUNT.labels(path='/predict', method='POST', status=400).inc()
                else:
                    self.send_response(404)
                    self.send_header('Content-Type', 'application/json')
                    self.end_headers()
                    self.wfile.write(json.dumps({"error": "Not found"}).encode())
                    REQUEST_COUNT.labels(path=self.path, method='POST', status=404).inc()
            except Exception as e:
                self.send_response(500)
                self.send_header('Content-Type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps({"error": str(e)}).encode())
                REQUEST_COUNT.labels(path=self.path, method='POST', status=500).inc()
            finally:
                REQUEST_LATENCY.observe(time.time() - start_time)

    def signal_handler(signum, frame):
        print('Received shutdown signal, exiting...')
        sys.exit(0)

    signal.signal(signal.SIGTERM, signal_handler)
    
    # Start Prometheus metrics server
    start_http_server(9090)
    print('Metrics server running on port 9090')
    
    # Initialize some metrics
    MODEL_MEMORY_USAGE.set(0)
    CACHE_HIT_RATIO.set(0)
    QUEUE_SIZE.set(0)
    
    # Start main server
    httpd = HTTPServer(('0.0.0.0', 8000), Handler)
    print('Server running on port 8000')
    httpd.serve_forever() 